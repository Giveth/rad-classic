{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '../tec-rewards/distribution_rounds/'\n",
    "rounds=np.arange(1,20)\n",
    "# load all the praise\n",
    "allpraise_df = pd.DataFrame()\n",
    "for kr in rounds:\n",
    "    df =pd.read_csv(f'{datadir}/round-{kr}/distribution_results/raw_csv_exports/extended_praise_data.csv')\n",
    "    allpraise_df=pd.concat([allpraise_df,df[['REASON','AVG SCORE','TO USER ACCOUNT','DATE']]],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>REASON</th>\n",
       "      <th>AVG SCORE</th>\n",
       "      <th>TO USER ACCOUNT</th>\n",
       "      <th>DATE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>for making edits in the welcome text</td>\n",
       "      <td>50.00</td>\n",
       "      <td>Suga#8514</td>\n",
       "      <td>2021-07-11T22:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for making edits in the welcome text.</td>\n",
       "      <td>4.67</td>\n",
       "      <td>Vyvy-vi#5040</td>\n",
       "      <td>2021-07-11T22:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>for offering to help us improve some designs f...</td>\n",
       "      <td>27.00</td>\n",
       "      <td>acidlazzer#5796</td>\n",
       "      <td>2021-07-11T22:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>for invite me to play some music</td>\n",
       "      <td>7.67</td>\n",
       "      <td>chuygarcia.eth#6692</td>\n",
       "      <td>2021-07-11T22:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>for sharing material about TEC simulator and c...</td>\n",
       "      <td>21.00</td>\n",
       "      <td>markop#2007</td>\n",
       "      <td>2021-07-11T22:00:00.000Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>for the initiative he is taking in so many fro...</td>\n",
       "      <td>6.97</td>\n",
       "      <td>GideonRo#3175</td>\n",
       "      <td>2022-10-07T14:10:31.607Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>for pulling together a list of influencial Twi...</td>\n",
       "      <td>22.25</td>\n",
       "      <td>bdegraf#7201</td>\n",
       "      <td>2022-10-07T19:03:57.358Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>for the session on TE Consilience Library from...</td>\n",
       "      <td>9.25</td>\n",
       "      <td>GideonRo#3175</td>\n",
       "      <td>2022-10-09T10:17:31.329Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>for continuing to focus design for next phase ...</td>\n",
       "      <td>15.00</td>\n",
       "      <td>stef#9877</td>\n",
       "      <td>2022-10-09T10:25:20.779Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>for continuing to focus TE Consilience Library...</td>\n",
       "      <td>8.82</td>\n",
       "      <td>stef#9877</td>\n",
       "      <td>2022-10-09T10:26:55.284Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15322 rows √ó 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                REASON  AVG SCORE  \\\n",
       "0                 for making edits in the welcome text      50.00   \n",
       "1                for making edits in the welcome text.       4.67   \n",
       "2    for offering to help us improve some designs f...      27.00   \n",
       "3                     for invite me to play some music       7.67   \n",
       "4    for sharing material about TEC simulator and c...      21.00   \n",
       "..                                                 ...        ...   \n",
       "136  for the initiative he is taking in so many fro...       6.97   \n",
       "137  for pulling together a list of influencial Twi...      22.25   \n",
       "138  for the session on TE Consilience Library from...       9.25   \n",
       "139  for continuing to focus design for next phase ...      15.00   \n",
       "140  for continuing to focus TE Consilience Library...       8.82   \n",
       "\n",
       "         TO USER ACCOUNT                      DATE  \n",
       "0              Suga#8514  2021-07-11T22:00:00.000Z  \n",
       "1           Vyvy-vi#5040  2021-07-11T22:00:00.000Z  \n",
       "2        acidlazzer#5796  2021-07-11T22:00:00.000Z  \n",
       "3    chuygarcia.eth#6692  2021-07-11T22:00:00.000Z  \n",
       "4            markop#2007  2021-07-11T22:00:00.000Z  \n",
       "..                   ...                       ...  \n",
       "136        GideonRo#3175  2022-10-07T14:10:31.607Z  \n",
       "137         bdegraf#7201  2022-10-07T19:03:57.358Z  \n",
       "138        GideonRo#3175  2022-10-09T10:17:31.329Z  \n",
       "139            stef#9877  2022-10-09T10:25:20.779Z  \n",
       "140            stef#9877  2022-10-09T10:26:55.284Z  \n",
       "\n",
       "[15322 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allpraise_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mitch/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# cleaning master function\n",
    "def clean_praise(praise):\n",
    "    # code adapted from: https://ourcodingclub.github.io/tutorials/topic-modelling-python/\n",
    "    my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "    word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem # clean words to the \"stem\" (e.g. words->word, talked->talk)\n",
    "    my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~‚Ä¢@'\n",
    "\n",
    "    praise = praise.lower() # lower case\n",
    "    praise = re.sub('['+my_punctuation + ']+', ' ', praise) # strip punctuation\n",
    "    praise = re.sub('\\s+', ' ', praise) #remove double spacing\n",
    "    praise = re.sub('([0-9]+)', '', praise) # remove numbers\n",
    "    praise_token_list = [word for word in praise.split(' ')\n",
    "                            if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    praise_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in praise_token_list] # apply word rooter\n",
    "\n",
    "    praise = ' '.join(praise_token_list)\n",
    "    return praise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean the language data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_praise = allpraise_df['REASON'].apply(clean_praise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the vectorizer object will be used to transform text to vector form\u001b[39;00m\n\u001b[1;32m      4\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, token_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m$[\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.]+|\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS+\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# remove words appear less than 5 times or more than 90%\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=5, token_pattern='\\w+|\\$[\\d\\.]+|\\S+') # remove words appear less than 5 times or more than 90%\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(cleaned_praise).toarray() # term frequency\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tf_feature_names) # total number of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 10\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_top_words = 10\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get for each praise which  topic does it belong to\n",
    "# Then, roughly check if the score relates to the praise\n",
    "\n",
    "# Finally, compare using just some keywords for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# just categorize by keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "among 15322 praises, 15043 have scores more than 0. Only include them. Next, clean them up.\n"
     ]
    }
   ],
   "source": [
    "nonzerodf = allpraise_df.loc[allpraise_df['AVG SCORE']>0]\n",
    "nonzerodf.insert(0,'CLEANED REASON',nonzerodf['REASON'].apply(clean_praise))\n",
    "\n",
    "print(f'among {len(allpraise_df)} praises, {len(nonzerodf)} have scores more than 0. Only include them. Next, clean them up.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import search # for searching sub strings\n",
    "type_keywords = {'attendance':'join|attend|show up|coming to','discussion':'question|ask|discuss|discussion|insight|participat|feedback|observations|forum|comment|share','work':'work|writing|hack|edit|studying|research|drafting|mediat|recording|taking notes|note taking|develop|analysis','lead':'host|lead|initiat|organizing|organizer|steward|forming|facilitate|managing','share':'share|spread','social media':'twitter|tweet|retweet|socials|social media','hardskills':'design|bug fixes|deploy|prototype|coding|python|javascript|solidity|data science','self-care':'vacation|time off|time-off|holiday|sabbatical|day off','IRL':'trip|DAOist|ETHcc|barcelona|denver|paris|amsterdam|colombia'}\n",
    "allcategs = []\n",
    "for kr,row in nonzerodf.iterrows():\n",
    "    category = []\n",
    "    praise = row['CLEANED REASON'].lower()\n",
    "    for praise_type,keywords in type_keywords.items():\n",
    "        if search(keywords,praise):\n",
    "            category.append(praise_type)\n",
    "    if len(category):\n",
    "        allcategs.append(category)\n",
    "    else:\n",
    "        allcategs.append(np.nan)\n",
    "category_df = pd.concat([nonzerodf.reset_index(), pd.DataFrame({\"category\":allcategs})],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5683 out of 15043 praises uncategorized\n"
     ]
    }
   ],
   "source": [
    "category_df.loc[category_df['category'].isnull()].to_csv('uncateogrized.csv')\n",
    "print(f\"{sum(category_df['category'].isnull())} out of {len(category_df)} praises uncategorized\")\n",
    "category_df.to_csv('categorized_praise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attendance                          join|attend|show up|coming to\n",
       "discussion      question|ask|discuss|discussion|insight|partic...\n",
       "work            work|writing|hack|edit|studying|research|draft...\n",
       "lead            host|lead|initiat|organizing|organizer|steward...\n",
       "share                                                share|spread\n",
       "social media           twitter|tweet|retweet|socials|social media\n",
       "hardskills      design|bug fixes|deploy|prototype|coding|pytho...\n",
       "self-care       vacation|time off|time-off|holiday|sabbatical|...\n",
       "IRL             trip|DAOist|ETHcc|barcelona|denver|paris|amste...\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(type_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis based on categorization\n",
    "When there's a praise matching more than one category, they will be counted multiple times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "categ_praise_scores = {k:[] for k in type_keywords.keys()}\n",
    "\n",
    "for kr,row in category_df.iterrows():\n",
    "    if type(row['category']) is list:\n",
    "        for key in row['category']:\n",
    "            categ_praise_scores[key] += [{'praise':row['REASON'],'avg_score':row['AVG SCORE'],'receiver':row['TO USER ACCOUNT'],'date':row['DATE']}]\n",
    "categ_praise_scores_df = dict.fromkeys(type_keywords.keys())\n",
    "for key, item in categ_praise_scores.items():\n",
    "    categ_praise_scores_df[key]= pd.DataFrame(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  the average, min, max score of each categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>max</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>attendance</th>\n",
       "      <td>3.181968</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>share</th>\n",
       "      <td>4.737731</td>\n",
       "      <td>73.33</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>social media</th>\n",
       "      <td>4.753523</td>\n",
       "      <td>47.75</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>discussion</th>\n",
       "      <td>7.699953</td>\n",
       "      <td>84.67</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>self-care</th>\n",
       "      <td>8.753333</td>\n",
       "      <td>25.33</td>\n",
       "      <td>2.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lead</th>\n",
       "      <td>10.267938</td>\n",
       "      <td>125.67</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>work</th>\n",
       "      <td>13.020339</td>\n",
       "      <td>125.67</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hardskills</th>\n",
       "      <td>14.173280</td>\n",
       "      <td>75.25</td>\n",
       "      <td>1.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IRL</th>\n",
       "      <td>18.201429</td>\n",
       "      <td>84.67</td>\n",
       "      <td>1.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   mean     max   min\n",
       "attendance     3.181968   73.33  0.03\n",
       "share          4.737731   73.33  0.10\n",
       "social media   4.753523   47.75  0.13\n",
       "discussion     7.699953   84.67  0.10\n",
       "self-care      8.753333   25.33  2.50\n",
       "lead          10.267938  125.67  0.13\n",
       "work          13.020339  125.67  0.20\n",
       "hardskills    14.173280   75.25  1.30\n",
       "IRL           18.201429   84.67  1.50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categ_stats = dict.fromkeys(type_keywords.keys())\n",
    "for categ in categ_praise_scores_df.keys():\n",
    "    categ_stats[categ] = {'mean':np.mean(categ_praise_scores_df[categ]['avg_score']),\n",
    "                            'max':np.max(categ_praise_scores_df[categ]['avg_score']),\n",
    "                            'min':np.min(categ_praise_scores_df[categ]['avg_score'])}\n",
    "categ_stats_df = pd.DataFrame(categ_stats)\n",
    "categ_stats_df.transpose().sort_values(by='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 3 highest scored praise in each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# attendance\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 73.33 | karmaticacid#1218 | for attending ETHDenver and doing presentations & panels about our work within the TEC | 2022-02-19\n",
       "| 55.0 | iviangita#3204 | for accepting the repsonisbility of acting Steward for the TEC and keeping tht WG afloat while we put out a call for new legal minds to join. | 2021-11-03\n",
       "| 27.5 | mattyjee#8621 | for having improved the reward system so much since he joined | 2022-03-29\n",
       "# discussion\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 84.67 | iviangita#3204 | for transcribing several months of praise given verbally during community calls -- a tedious and thankless task that brought a lot more praise into the system than would otherwise be ingested. I think that volume of praise input has helped us to better understand the system and will help to iterate it. | 2022-05-27\n",
       "| 84.67 | natesuits#4789 | for helping me write and rewrite the forum post on the power of defaults: https://forum.tecommons.org/t/the-power-of-defaults-in-the-commons-configuration-dashboard/511/2 | 2021-07-25\n",
       "| 80.33 | Nuggan#5183 | for helping me write and rewrite the forum post on the power of defaults: https://forum.tecommons.org/t/the-power-of-defaults-in-the-commons-configuration-dashboard/511/2 | 2021-07-25\n",
       "# work\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 125.67 | sem(üå∏,üêù)#0161 | for developing and testing the smart contracts. They are actually hosting a demo of the augmented bonding curve and all the commons upgrade tooling. Much admiration and respect for that | 2021-11-28\n",
       "| 102.75 | wslyvh#1059 | for single handedly developing Tokenlog which the TEC has relied on for so many of our decisioning | 2022-05-19\n",
       "| 89.0 | kristofer#1475 | for their front-end and back-end development work on the praise dashboard | 2022-04-21\n",
       "# lead\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 125.67 | sem(üå∏,üêù)#0161 | for developing and testing the smart contracts. They are actually hosting a demo of the augmented bonding curve and all the commons upgrade tooling. Much admiration and respect for that | 2021-11-28\n",
       "| 68.0 | mZ#3472 | for thought leadership in web3 and for keeping engineering ethics as our community‚Äôs North Star | 2021-12-25\n",
       "| 63.5 | elessar.eth#7945 | for developing and testing the smart contracts. They are actually hosting a demo of the augmented bonding curve and all the commons upgrade tooling. Much admiration and respect for that | 2021-11-28\n",
       "# share\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 73.33 | sem(üå∏,üêù)#0161 | for the incredible work with the demos, making improvements, managing the tech team and for being such a good teacher across the space and sharing his knowledge | 2021-07-15\n",
       "| 68.0 | sem(üå∏,üêù)#0161 | for engaging in discussion on the TE Commons Forum (https://forum.tecommons.org) the past week. Thank you for helping our Token Engineering Commons community share and learn! | 2021-07-15\n",
       "| 59.33 | sem(üå∏,üêù)#0161 | for the Real Time Launch action! Great sharing the war room with you!!! | 2022-01-24\n",
       "# social media\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 47.75 | iviangita#3204 | for finding out our twitter account as block and fix it super quick!!‚ö° (We need to be carefull with bots) | 2021-09-08\n",
       "| 42.0 | iviangita#3204 | for all the incredible behind-the-scenes work and all the little things she‚Äôs constantly doing in the back office helping with Twitter, the board and work agreements | 2021-09-02\n",
       "| 33.33 | innov8tor3#3988 | for mentioning or retweeting TE Commons on the socials the past week! Thank you for helping us grow the Token Engineering Commons community and spreading the message! üôèüèº‚ò∫Ô∏è | 2021-07-15\n",
       "# hardskills\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 75.25 | Mert Ozd#6679 | for product managing the proposal inverter, including faciliating the weekly meetings, working through UX and designs issues and making sure payments are organized | 2022-05-25\n",
       "| 63.67 | VitorNunes#0090 | for all the work on the CCD, the designs, comments, user testing and making things understandable for people | 2021-08-19\n",
       "| 57.67 | sem(üå∏,üêù)#0161 | for working overtime seeming all the time to get all the commons upgrade pieces deployed | 2022-01-20\n",
       "# self-care\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 25.33 | Suga#8514 | for getting out the newsletter when there's been a huge fire in the area, she had nowhere to stay, and she's on holiday with her family, and for making sure the AMA was covered. Above and beyond! | 2021-08-03\n",
       "| 15.75 | markop#2007 | for being so supportive across the commons. I've witnessed in my travels many occurrences of resources and time offered to community members in a handful of channels. <a:culture:770033606080856094> | 2021-11-26\n",
       "| 12.75 | griff (üíú, üíú)#8888 | for showing up to so many meetings even though they‚Äôre on holiday | 2021-11-25\n",
       "# IRL\n",
       "    | Avg. score | To | Reason | Date |\n",
       "     |:-----------|----|:-------|\n",
       "| 84.67 | Tam2140#9361 | Great 18 minute talk at ETH Denver (https://www.youtube.com/watch?v=7UvPCDbBKcQ). You must've prepped for hours for the talk and the slides you prepared. | 2022-04-22\n",
       "| 82.0 | Tam2140#9361 | for supporting to accomodate people going to Amsterdam for Devconnect. Your clarity really helped us feel more calm regarding our staying plans. | 2022-04-11\n",
       "| 76.5 | chuygarcia.eth#6692 | for being the TEC fam who went to Amsterdam and represented our Commons during all those events | 2022-05-04\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "mdtext = ''\n",
    "for categ in categ_praise_scores_df.keys():\n",
    "    categ_name = '# '+categ + '\\n'\n",
    "    toppraise = categ_praise_scores_df[categ].sort_values(by='avg_score',ascending=False).iloc[:3]\n",
    "    top3_table= (f\"\\\n",
    "    | Avg. score | To | Reason | Date |\\n \\\n",
    "    |:-----------|----|:-------|\\n\")\n",
    "    for kr,row in toppraise.iterrows():\n",
    "        to_user = row['receiver']\n",
    "        reason = row['praise']\n",
    "        score = row['avg_score']\n",
    "        date = row['date'][:10]\n",
    "                    \n",
    "        top3_table += (f\"| {score} | {to_user} | {reason} | {date}\\n\")\n",
    "        #print(f'Praise score average: {score}\\nFROM {from_user} TO {to_user},reason:\\n{reason}\\n')\n",
    "    mdtext += categ_name + top3_table    \n",
    "md(mdtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- maybe further adjust keyword to make the top scores look normal\n",
    "- how to make keywords a manipulable setting in json?\n",
    "- incorporate this into cross-period analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
